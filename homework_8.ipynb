{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Introduction to Neural Networks\"\n",
    "collection: publications\n",
    "permalink: /notebooks/neuralnets_1\n",
    "excerpt: 'This notebook introduces the use of neural networks via classification tasks. EE PMP 559, Spring 2019'\n",
    "date: 2019-05-21\n",
    "paperurl: 'https://github.com/cpatdowling/ee559'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks (NNs) have gained massive popularity due to their recent successes in many applications. NNs can achieve human-comparable accuracy in tasks like image recognition, but they are not a new concept. The recent notariety stems from advances in computing power and the availability of _large enough amounts_ of useful data on which to train have improved their applicability in industrial and research applications. In this notebook we'll take a bird's eye view over what exactly a NN is and how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> Fig. 1: Our example deep neural network</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt](figs/NN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fig. 1 illustrates a typical NN architecture used in classification. The input vector is the far left column, which each feature of the sample (in all of our homeworks, a row of a data matrix) is treated as the input. $W_1$ is a weight matrix used in each of the nodes of the first layer of the NN, $G_{1}$. $G_{1}$ is a function---like a sigmoid (like we saw in logistic regression)---that operates on $w^{T}x$. In the case of a sigmoid like we saw in logistic regression:\n",
    "\n",
    "$G_{1} = \\frac{1}{1 + e^{-w^{T}x}}$\n",
    "\n",
    "Each node of the first layer is a single column vector of the weight matrix---each layer can have higher or lower numbers of dimensions $m$. The outputs of each node are used as inputs for the next node, creating a long chain of composed activation functions, or layers, $G_{k}$. As $k$ grows large, a neural network is said to be \"deep\". At the end of the day, a neural network is just a long composition of special classes of functions:\n",
    "\n",
    "$\\hat{y} = G_{k}(G_{k-1}(\\ldots G_{2}(G_{1}(x, W_{1}), W_{2})\\ldots ), W_{k})$\n",
    "\n",
    "Part of what makes training a NN feasible is that we choose differentiable functions for $G$. We can apply gradient descent, like we did in early homeworks. Suppose we train a NN with L2-loss, then:\n",
    "\n",
    "$\\mathcal{L}(\\hat{y},y) = \\| y - \\hat{y} \\|_{2}^{2}$\n",
    "\n",
    "$\\mathcal{L}(\\hat{y},y) = \\| y - G_{k}(G_{k-1}(\\ldots G_{2}(G_{1}(x, W_{1}), W_{2})\\ldots ), W_{k}) \\|_{2}^{2}$\n",
    "\n",
    "The derivative of the loss $\\mathcal{L}$ with respect to the weights $W_{1}, \\ldots, W_{k}$ between each of the NN layers can be found by application of the chain rule, boiling down to an excercise in recording-keeping the indecies. The _backpropogation_ algorithm applies gradient descent updates to a multi-layer NN. What makes a NN so powerful is that it can, in theory, approximate _any_ function. In practice, this can A) require an infeasible amount of _labeled_ data and B) easily lead to overfitting if not carefully tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a high level, NNs elegantly combine two of the concepts we've learned so far: regression and the kernel method. In the real world, things we want to learn from data are too _high dimensional_---meaning that the output we care about is a function of many, many features---and as we saw in previous homeworks, fitting a line to seperate data, or follow the trend of data quickly becomes difficult as we add dimensions and non-linear bases.\n",
    "\n",
    "A NN attempts to do this programmatically where each layer transforms the data: either by expanding the dimensions like we saw with the SVM kernel trick (making $m$ bigger with each layer), and by finding lines of seperation (by minimizing the weights $w$ with gradient descent). In this assignment, we'll walk through using a package called PyTorch---the Python extension of Torch. There are tons of [accessible tutorials](https://pytorch.org/tutorials/) in doing some very cool ML tasks with PyTorch. The tutorials come in the [form of notebooks](https://pytorch.org/tutorials/_downloads/17a7c7cb80916fcdf921097825a0f562/cifar10_tutorial.ipynb) like this one.\n",
    "\n",
    "Another option would be Google's recently released TensorFlow v2.0. While the newest version attempts to improve on some of the vagueries of v1.0, TensorFlow is more difficult to learn, interpret, and prototype with. Its biggest advantages are in late-stage algorithm deployment within an enterprise software stack; while we aren't doing that here, if your goal is to be a ML software engineer, TensorFlow is worth learning. \n",
    "\n",
    "Torch benefits over TensorFlow by being an object oriented implementation within an object oriented language like Python. Optimizers, loss, and activiation functions are easy enough to use that all of the ML tasks performed in our previous assignments have straightforward Torch implementations."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#TO INSTALL\n",
    "#Detailed installation Instructions: https://pytorch.org/get-started/locally/\n",
    "\n",
    "#Having downloaded Anaconda, open a terminal/command prompt and type:\n",
    "\n",
    "conda install pytorch torchvision -c pytorch\n",
    "\n",
    "#Note: Mac does not natively support CUDA. If you want to run these networks on your GPU, you'll need to install from source. We will not be using CUDA impelementations in this walkthrough, so this is not required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This homework assignment will be running cells and making small changes to this walkthrough. Most of the challenge will likely come from getting PyTorch installed, if problems do arise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1 (Load and view the data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to jump right into training a classifier using the CIFAR10 dataset. These questions follow along the lines of the tutorial found [here](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py). In this environment, each image is a sample, and the NN's job is to classify the image with a label of what's in the image. Below are the 10 classes the images fall into:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt](figs/cifar10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sample is a single color (very small) image. We transform the pixel Red-Green-Blue color values into a long vector and pass this vector into the NN as input. The output is one of the ten categories. After installing PyTorch, run each of the cells below to train and view the performance of the NN. A much larger and more daunting image classification task; labeling images in the ImageNet dataset, can be found [here](https://github.com/pytorch/examples/tree/master/imagenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import the required libraries\n",
    "import torch       #base library\n",
    "import torchvision        #library for image classification/computer vision tasks\n",
    "import torchvision.transforms as transforms       #utilities for transforming data matrices\n",
    "import torch.nn as nn            #this library contains activation layer and loss functions\n",
    "import torch.nn.functional as F  #this contains weight adjusting functions\n",
    "import torch.optim as optim      #this library contains optimizers like gradient descent\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "#this process is going to create a new local folder to store the data downloaded from the web in\n",
    "#here we load the train data, already saved as a common baseline task for new image recognition algorithms\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=1)\n",
    "\n",
    "#when training a NN you should 1) shuffle the order of your training data, and 2) create mini-batches of the data\n",
    "#and perform gradient descent on each batch 1 at a time. The \"num_workers\" argument allows you to\n",
    "#paralellize the task of reading in the data with as many cores as your computer has; I've set this to 1\n",
    "#but setting it to 2 or 4 can speed things up\n",
    "\n",
    "#here we load the train data, these loaders are fancy wrappers for making data processing easier, but ultimately\n",
    "#these are just matrices with a bunch of rows, each corresponding to a single sample\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=1)\n",
    "\n",
    "#these are the labels for classifying the data samples\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we can inspect the images and their cooresponding labels.\n",
    "#images are stored in a height x width x 3 matrix. The 3 refers to the Red-Green-Blue color channel values\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
    "print(\"Look at these absurdly small images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2 (Define the NN architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cells below we define the NN architecture. The important thing to note here is that in the init(self) routine, each line below that is a __layer__ of the network. I've commented out the line self.fc2 = nn.Linear(120, 84). Each linear layer has the form \"nn.Linear(input_dimension, output_dimension).\n",
    "\n",
    "The input dimension of the current layer must match the output dimension of the previous layer. Notice that the output dimension of self.fc1 matches the input dimension of self.fc3. Later on you will adjust the network and test the performance with an additional linear layer. Run the following cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#below we're going to design a simple \n",
    "\n",
    "#here we define the class Net that lets us move around adjust the network in memory like any other data structure\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)   #each of these self.function lines is a layer of the network\n",
    "        self.pool = nn.MaxPool2d(2, 2)        #each of these layers performs a specific task in transforming the image\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        #self.fc2 = nn.Linear(120, 84)          #we're going to focus on these linear layers\n",
    "        self.fc3 = nn.Linear(120, 10)           #the format is nn.Linear(input_dimension, output_dimension)\n",
    "\n",
    "    #this defines the function composition G_k(G_k-1(... G_1(x,w_1), w_2)... )))\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This loss function is applied to the outputs of the Net's forward operation, comparing the prediction\n",
    "#y_hat to the true output label y\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "#this encodes gradient descent: notice two important parameters\n",
    "#1) the learning rate determines initially how big of a step the gradient descent algorithm takes\n",
    "#2) momentum <1 encourages the gradient descent algorithm to slow down as it takes more and more steps\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3 (Train and test the network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step can take a while depending on how powerful your computer is, but shouldn't take up too much memory. If your computer is unable to handle the training stage, run the cell and keep the error output as your answer here. You can speed up the process by increasing the num_worker argument in the trainloader above to half the number of CPU cores you have available. If you're interested (not required), the CUDA implementation details can be found at the bottom of the tutorial [here](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py). Convolutional NN's like the one we use here can be sped up significantly by using GPU's instead of CPU's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):  #this trains the algorithm for each batched training set\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients // important for backpropogation updates to work\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        y_hat = net(inputs)\n",
    "        loss = loss_func(y_hat, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we compute the general accuracy for each of the 10 image classes\n",
    "\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4 (Improve the Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the code and Net class above and uncomment the following lines in the Net class:\n",
    "\n",
    "self.fc2(120, 84)\n",
    "x = F.relu(self.fc2(x))\n",
    "\n",
    "Adjust the output dimension of self.fc1 accordingly. Train a new NN:\n",
    "\n",
    "net2 = Net()\n",
    "\n",
    "with the additional layer and compare the test general accuracy. Which classes performed better or worse? In your own words, comment on how you might programmatically search NN architectures to find the best test performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#copy the net class, the optimizer and loss function declaration here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#copy the train and test routines here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#insert remaining code/comments here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
